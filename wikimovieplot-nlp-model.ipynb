{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Maagnitude/wikimovieplot-nlp-model/blob/main/wikimovieplot-nlp-model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3η Εργασία** στο μάθημα **Μηχανική Μάθηση και Εφαρμογές**\n",
        "\n",
        "# **Τμήμα Πληροφορικής και Τηλεματικής - Χαροκόπειο Πανεπιστήμιο**\n",
        "\n",
        "# **Καζάζης Γεώργιος - it214124**\n",
        "\n",
        "Στην παρούσα εργασία θα χρησιμοποιήσουμε μεθόδους επεξεργασίας φυσικής γλώσσας."
      ],
      "metadata": {
        "id": "C8kx-GPaYPi3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Modules**\n",
        "Κάνουμε import τις απαραίτητες βιβλιοθήκες:\n",
        "* **pandas** και **numpy** για την διαχείριση των δεδομένων μας.\n",
        "* **re** για την επεξεργασία κειμένου (lowercase κτλ.)\n",
        "* **tensorflow**, και από αυτήν το **keras** καθώς και τα **layers**, **losses**, **preprocessing** για την ανάπτυξη Νευρωνικών Δικτύων, και την εκπαίδευση τους.\n",
        "* **TextVectorization** για να κάνουμε **vectorize** τα **Plot** και **Title** για την χρήση τους στην εκπαίδευση.\n",
        "* Τα **warnings** για να τα φιλτράρουμε, ώστε να μην εμφανίζονται "
      ],
      "metadata": {
        "id": "5S6lZwror07D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "from keras import losses\n",
        "from keras import preprocessing\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(action='ignore')"
      ],
      "metadata": {
        "id": "4P5qV3xhYshF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Dataset**\n",
        "Περνάμε το link από το **public github repo** μου, για το **dataset** μας, σε μορφή **csv**, το οποίο θα δώσουμε στην συνάρτηση για το φόρτωμα των δεδομένων μας."
      ],
      "metadata": {
        "id": "VoalJpu0avWb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://raw.githubusercontent.com/Maagnitude/wikimovieplot-nlp-model/main/dataset.csv\""
      ],
      "metadata": {
        "id": "CEruND9PauoA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Text preprocessing**\n",
        "Με αυτή την συνάρτηση θα επεξεργαστούμε τα Title και Plot, ώστε να έχουν όλα πεζά γράμματα, να μην έχουν ειδικούς χαρακτήρες και σημεία στίξης."
      ],
      "metadata": {
        "id": "flTnjrrhtn00"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    text = text.lower() # convert text to lowercase\n",
        "    text = re.sub(r'[^\\w\\s]', '', text) # remove punctuation\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    return text"
      ],
      "metadata": {
        "id": "nt6skjaysvuh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Βασική συνάρτηση preprocessing**\n",
        "Εδώ υλοποιείται το μεγαλύτερο κομμάτι του notebook. \n",
        "\n",
        "Αρχικά φορτώνεται το dataset, ανακατεύονται οι γραμμές του με την sample, χωρίζεται σε train, validate και test set (έχοντας υπολογίσει τα ποσοστά του καθενός).\n",
        "\n",
        "Εν συνεχεία, γίνεται **One-hot encoding** στα features '**Origin/Ethnicity**' και '**Genre**' με την χρήση της **pd.get_dummies** (σε κάθε **set**), τα οποία περνιούνται με τα κατάλληλα **keys** στα **dictionaries** που δημιουργούμε στο επόμενο βήμα. Από το shape[1] ενός εκ των τριών genre_dummies, παίρνουμε τις 20 διαφορετικές τιμές του feature 'Genre' ώστε να το χρησιμοποιήσουμε αργότερα.\n",
        "\n",
        "Πριν περάσουμε τα '**Title**' και '**Plot**' στα **dictionaries**, τα περνάμε από την παραπάνω συνάρτηση επεξεργασίας κειμένου με την χρήση της **apply()**. \n",
        "\n",
        "Δημιουργούμε τα **dictionaries** και ύστερα τα περνάμε με την **from_tensor_slices()** (των **tf.data.Dataset**) στα Dataset object (**raw_train_ds**, **raw_val_ds**, **raw_test_ds**), και τέλος τα επιστρέφουμε. "
      ],
      "metadata": {
        "id": "TmjY6-Jut9JH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data_wiki(file_path, batch_size=32, p_train=0.65, p_val=0.15):\n",
        "    \n",
        "    df = pd.read_csv(url)\n",
        "    df = df.sample(frac=1, random_state=42)\n",
        "\n",
        "    n = df.shape[0]\n",
        "    n_train = int(n * p_train)\n",
        "    n_val = int(n * p_val)\n",
        "    n_test = n - n_train - n_val\n",
        "\n",
        "    df_train, df_val, df_test = df.iloc[:n_train], df.iloc[n_train:n_train+n_val],\\\n",
        "                                                          df.iloc[n_train+n_val:]\n",
        "\n",
        "    origin_dummies1 = pd.get_dummies(df_train['Origin/Ethnicity'])\n",
        "    genre_dummies1 = pd.get_dummies(df_train['Genre'])\n",
        "\n",
        "    origin_dummies2 = pd.get_dummies(df_val['Origin/Ethnicity'])\n",
        "    genre_dummies2 = pd.get_dummies(df_val['Genre'])\n",
        "\n",
        "    origin_dummies3 = pd.get_dummies(df_test['Origin/Ethnicity'])\n",
        "    genre_dummies3 = pd.get_dummies(df_test['Genre'])\n",
        "\n",
        "    n_labels = genre_dummies1.shape[1]\n",
        "\n",
        "    df['Title'] = df['Title'].apply(preprocess_text)\n",
        "    df['Plot'] = df['Plot'].apply(preprocess_text)  \n",
        "                                                            \n",
        "    train_data = {'Title': df_train['Title'].values,\n",
        "                  'Origin': origin_dummies1.values.astype(np.float64),\n",
        "                  'Plot': df_train['Plot'].values,\n",
        "                  'Genre': genre_dummies1.values.astype(np.float64)}\n",
        "\n",
        "    val_data = {'Title': df_val['Title'].values,\n",
        "                'Origin': origin_dummies2.values.astype(np.float64),\n",
        "                'Plot': df_val['Plot'].values,\n",
        "                'Genre': genre_dummies2.values.astype(np.float64)}            \n",
        "\n",
        "    test_data = {'Title': df_test['Title'].values,\n",
        "                 'Origin': origin_dummies3.values.astype(np.float64),\n",
        "                 'Plot': df_test['Plot'].values,\n",
        "                 'Genre': genre_dummies3.values.astype(np.float64)}\n",
        "\n",
        "    raw_train_ds = tf.data.Dataset.from_tensor_slices(train_data).batch(batch_size)\n",
        "    raw_val_ds = tf.data.Dataset.from_tensor_slices(val_data).batch(batch_size)\n",
        "    raw_test_ds = tf.data.Dataset.from_tensor_slices(test_data).batch(batch_size)\n",
        "\n",
        "    return raw_train_ds, raw_val_ds, raw_test_ds, n_labels"
      ],
      "metadata": {
        "id": "51196DXOH9PA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_train_ds, raw_val_ds, raw_test_ds, n_labels = load_data_wiki(url)"
      ],
      "metadata": {
        "id": "5DpaPDLAJzXM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Έλεγχος των δεδομένων**\n",
        "Εδώ θα τσεκάρουμε τον **αριθμό των διαφορετικών Genre**, καθώς και αν τα **Title**, **Genre**, **Origin** και **Plot**, της πρώτης ταινίας του πρώτου batch, έχουν περαστεί σωστά (**Title** και **Plot** επεξεργασμένα σωστά - **Origin** και **Genre** **one-hot encoded**)"
      ],
      "metadata": {
        "id": "8i3H0kTwwbTR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_labels"
      ],
      "metadata": {
        "id": "hi5K3q1eKiv0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for movie in raw_train_ds.take(1):\n",
        "  print('First movie, from the first batch:')\n",
        "  print('Title (lowercase etc.): ', movie['Title'][0].numpy())\n",
        "  print('Genre (One-hot): ', movie['Genre'][0].numpy())\n",
        "  print('Origin (One-hot): ', movie['Origin'][0].numpy())\n",
        "  print('Plot (lowercase etc.): ', movie['Plot'][0].numpy())\n"
      ],
      "metadata": {
        "id": "Hwv4KMBdMRie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Πρώτο Vectorization**\n",
        "Εδώ θα υλοποιήσουμε μία διαδικασία η οποία κάνει vectorize τα **Title** και **Plot**, ώστε να δημιουργηθεί μια δυαδική αναπαράσταση των λέξεων (**bag of words**) με λεξιλόγιο (αριθμό λέξεων) που θα ορίσουμε εμείς.\n",
        "\n",
        "* Το πρώτο λεξιλόγιο θα έχει **500 λέξεις** (**max_features**). \n",
        "* Το **output** θα είναι **binary** (1 αν υπάρχει η λέξη, 0 αν δεν υπάρχει)\n",
        "* Με το **pad_to_max_tokens**, βάζοντας το **True**, του λέμε να συμπληρώσει με **μηδενικά** έως ότου οι λέξεις να είναι όσο το **max_features** που θα του δώσουμε."
      ],
      "metadata": {
        "id": "eJfZZrNYyage"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_features = 500\n",
        "\n",
        "vectorize_layer = TextVectorization(\n",
        "    max_tokens=max_features,\n",
        "    output_mode='binary',\n",
        "    pad_to_max_tokens=True)"
      ],
      "metadata": {
        "id": "abj6ztx1rQJS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorize_text(text):\n",
        "  return vectorize_layer(tf.expand_dims(text, -1))"
      ],
      "metadata": {
        "id": "a2BNLFC-Z_EW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Χρήση του Title**\n",
        "Για αρχή χρησιμοποιούμε μόνο το **title** ως είσοδο (χωρίς το Plot), και το δίνουμε όπως και το κάνουμε **adapt** στο **vectorize_layer**, ώστε παρακάτω να γίνει **vectorized** πριν δωθεί στα τελικά sets."
      ],
      "metadata": {
        "id": "IPiHGP6JzqgU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def final_sets(raw_train_ds, raw_val_ds, raw_test_ds):\n",
        "    train_titles = raw_train_ds.map(lambda x: x['Title'])\n",
        "    vectorize_layer.adapt(train_titles)\n",
        "    val_titles = raw_val_ds.map(lambda x: x['Title'])\n",
        "    vectorize_layer.adapt(val_titles)\n",
        "    test_titles = raw_test_ds.map(lambda x: x['Title'])\n",
        "    vectorize_layer.adapt(test_titles)\n",
        "\n",
        "    train_ds = raw_train_ds.map(lambda x: (vectorize_text(x['Title']), x['Genre']))\n",
        "    val_ds = raw_val_ds.map(lambda x: (vectorize_text(x['Title']), x['Genre']))\n",
        "    test_ds = raw_test_ds.map(lambda x: (vectorize_text(x['Title']), x['Genre']))\n",
        "\n",
        "    return train_ds, val_ds, test_ds"
      ],
      "metadata": {
        "id": "CnGLTietZ44c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds, val_ds, test_ds = final_sets(raw_train_ds, raw_val_ds, raw_test_ds)"
      ],
      "metadata": {
        "id": "t56Y9SeVeYZf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Πρώτο μοντέλο** \n",
        "Γίνεται η εκπαίδευση του πρώτου μοντέλου με χρήση μόνο του **Title**, για την πρόβλεψη του **Genre**.\n",
        "\n",
        "**10020 παράμετροι** - **Είσοδος = 500**,  **Έξοδος = 20**"
      ],
      "metadata": {
        "id": "phcAspLw3MIv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential([layers.Dense(20, activation='sigmoid', input_shape=(max_features,))])\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "QojWc9OIYnbr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Αποτελέσματα**\n",
        "Χρησιμοποιήθηκε ως **optimizer** η μέθοδος **Adam**, με **learning rate** **0.001**, και **default** οι υπόλοιπες παράμετροι.\n",
        "\n",
        "Στις 10 εποχές είχαμε **Accuracy: 0.303**, και **Loss: 0.161**\n",
        "\n",
        "Στις 20 εποχές είχαμε **Accuracy: 0.316**, και **Loss: 0.158**\n",
        "\n",
        "**Δεν δοκιμάστηκε** σε παραπάνω εποχές μιας και θα το ξανατρέξουμε για **λεξιλόγιο 10000 λέξεων** ώστε να δούμε αν υπάρχει κάποια διαφορά. "
      ],
      "metadata": {
        "id": "pjiBU_6a3pEP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss=losses.BinaryCrossentropy(),\n",
        "              optimizer=tf.keras.optimizers.Adam(0.001),\n",
        "              metrics=['accuracy'])\n",
        "epochs = 20\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=epochs)\n",
        "\n",
        "loss, accuracy = model.evaluate(test_ds)\n",
        "\n",
        "print(\"Loss: \", loss)\n",
        "print(\"Accuracy: \", accuracy)"
      ],
      "metadata": {
        "id": "2byKIQvmYqnT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Λεξιλόγιο 10000 λέξεων**\n",
        "Αυξάνουμε τις λέξεις στο **bag of words**, και ξαναδημιουργούμε τα **sets**. Ύστερα εκπαιδεύουμε πάλι το μοντέλο μας και το τρέχουμε με τις **ίδιες παραμέτρους** για **20 εποχές**, για να το συγκρίνουμε με το **πρώτο**.\n",
        "\n",
        "Πλέον, έχουμε **200020 παραμέτρους**, **Είσοδο=10000**, **Έξοδο=20**."
      ],
      "metadata": {
        "id": "uZDFhmPs7CCN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_features = 10000\n",
        "vectorize_layer = TextVectorization(\n",
        "    max_tokens=max_features,\n",
        "    output_mode='binary',\n",
        "    pad_to_max_tokens=True)\n",
        "\n",
        "train_ds, val_ds, test_ds = final_sets(raw_train_ds, raw_val_ds, raw_test_ds)"
      ],
      "metadata": {
        "id": "ztD-Clyc4_GZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential([layers.Dense(20, activation='sigmoid', input_shape=(max_features,))])\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "cVY_j5ip6lCt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Αποτελέσματα 2**\n",
        "Στις **20 εποχές** έχουμε **Accuracy: 0.355** και **Loss: 0.159**.\n",
        "\n",
        "\n",
        "**Απάντηση στην ερώτηση**:\n",
        "\n",
        "Τα έχει πάει σίγουρα καλύτερα από το πρώτο, αλλά μεγάλη διαφορά δεν υπάρχει παρ' όλο που αυξήθηκαν οι λέξεις στο λεξιλόγιο, κι αυτό οφείλεται, κατά πάσα πιθανότητα, στο ότι με τον τίτλο μόνο, το μοντέλο δεν μπορεί να πάρει τις απαραίτητες πληροφορίες στην εκπαίδευση του. "
      ],
      "metadata": {
        "id": "d3Oq2R_E724W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss=losses.BinaryCrossentropy(),\n",
        "              optimizer=tf.keras.optimizers.Adam(0.001),\n",
        "              metrics=['accuracy'])\n",
        "epochs = 20\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=epochs)\n",
        "\n",
        "loss, accuracy = model.evaluate(test_ds)\n",
        "\n",
        "print(\"Loss: \", loss)\n",
        "print(\"Accuracy: \", accuracy)"
      ],
      "metadata": {
        "id": "BGTTFDYQ6pKP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Χρήση του Plot**\n",
        "Τώρα με την χρήση της συνάρτησης **final_sets_plot**, θα δημιουργήσουμε νέα sets τα οποία θα έχουν μόνο το **Plot**, ώστε να δοκιμάσουμε να εκπαιδεύσουμε ένα μοντέλο με τα **σενάρια των ταινιών** και να ελέγξουμε τα αποτελέσματα του."
      ],
      "metadata": {
        "id": "-A946HbZ9Xxi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def final_sets_plot(raw_train_ds, raw_val_ds, raw_test_ds):\n",
        "    train_titles = raw_train_ds.map(lambda x: x['Plot'])\n",
        "    vectorize_layer.adapt(train_titles)\n",
        "    val_titles = raw_val_ds.map(lambda x: x['Plot'])\n",
        "    vectorize_layer.adapt(val_titles)\n",
        "    test_titles = raw_test_ds.map(lambda x: x['Plot'])\n",
        "    vectorize_layer.adapt(test_titles)\n",
        "\n",
        "    train_ds = raw_train_ds.map(lambda x: (vectorize_text(x['Plot']), x['Genre']))\n",
        "    val_ds = raw_val_ds.map(lambda x: (vectorize_text(x['Plot']), x['Genre']))\n",
        "    test_ds = raw_test_ds.map(lambda x: (vectorize_text(x['Plot']), x['Genre']))\n",
        "\n",
        "    return train_ds, val_ds, test_ds"
      ],
      "metadata": {
        "id": "IDMWYUT08yLL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Θα χρησιμοποιήσουμε ένα **λεξιλόγιο 1000 λέξεων**, και όλες τις υπόλοιπες παραμέτρους ίδιες."
      ],
      "metadata": {
        "id": "ts_BTD7G9sx7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_features = 1000\n",
        "vectorize_layer = TextVectorization(\n",
        "    max_tokens=max_features,\n",
        "    output_mode='binary',\n",
        "    pad_to_max_tokens=True)\n",
        "\n",
        "train_ds, val_ds, test_ds = final_sets_plot(raw_train_ds, raw_val_ds, raw_test_ds)"
      ],
      "metadata": {
        "id": "G_G5Azwh8_4E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Παράμετροι=20020**, **Είσοδος=1000**, **Έξοδος=20**"
      ],
      "metadata": {
        "id": "MVoWKO3K-BOb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential([layers.Dense(20, activation='sigmoid', input_shape=(max_features,))])\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "o_TXOvmh9F1-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Αποτελέσματα 3**\n",
        "Έχουμε **καθαρή βελτίωση** από τα προηγούμενα, μιας και πλέον έχει ως είσοδο τα **σενάρια** των ταινιών από τα οποία μπορεί να αντλήσει σαφώς περισσότερες πληροφορίες, κι έτσι με **λεξιλόγιο 1000 λέξεων** τα πήγε πολύ καλύτερα από ότι τα είχε πάει το μοντέλο που εκπαιδεύτηκε στους **τίτλους** των ταινιών με **λεξιλόγιο 10000 λέξεων**.\n",
        "\n",
        "**Accuracy: 0.454**, **Loss: 0.139**\n",
        "\n",
        "Παρατηρούμε όμως ότι μετά την **7η εποχή** το **val_loss** μόνο αυξάνεται και γενικά ενώ το μοντέλο πετυχαίνει **accuracy** στο train set τιμές υψηλότερες του **0.57**, δεν γενικεύει αποτελεσματικό, περίπτωση **υπερεκπαίδευσης**. Θα γίνει μία δοκιμή και στις **10 εποχές**."
      ],
      "metadata": {
        "id": "fM3wcUhy-KfA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss=losses.BinaryCrossentropy(),\n",
        "              optimizer=tf.keras.optimizers.Adam(0.001),\n",
        "              metrics=['accuracy'])\n",
        "epochs = 20\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=epochs)\n",
        "\n",
        "loss, accuracy = model.evaluate(test_ds)\n",
        "\n",
        "print(\"Loss: \", loss)\n",
        "print(\"Accuracy: \", accuracy)"
      ],
      "metadata": {
        "id": "4Y8aqqA-9Jkc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Εκπαίδευση με 10 εποχές για αποφυγή υπερεκπαίδευσης**\n",
        "Πράγματι έχουμε **Accuracy: 0.463**, και **Loss: 0.131**. Υπάρχει μικρή βελτίωση, όχι αξιοσημείωτη."
      ],
      "metadata": {
        "id": "KNoZ_kSLBH8B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_features = 1000\n",
        "vectorize_layer = TextVectorization(\n",
        "    max_tokens=max_features,\n",
        "    output_mode='binary',\n",
        "    pad_to_max_tokens=True)\n",
        "\n",
        "train_ds, val_ds, test_ds = final_sets_plot(raw_train_ds, raw_val_ds, raw_test_ds)"
      ],
      "metadata": {
        "id": "VY_dfN5bBViE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential([layers.Dense(20, activation='sigmoid', input_shape=(max_features,))])\n",
        "model.summary()\n",
        "model.compile(loss=losses.BinaryCrossentropy(),\n",
        "              optimizer=tf.keras.optimizers.Adam(0.001),\n",
        "              metrics=['accuracy'])\n",
        "epochs = 10\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=epochs)\n",
        "\n",
        "loss, accuracy = model.evaluate(test_ds)\n",
        "\n",
        "print(\"Loss: \", loss)\n",
        "print(\"Accuracy: \", accuracy)"
      ],
      "metadata": {
        "id": "JZOlOLbkBE8N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_features = 30000\n",
        "vectorize_layer = TextVectorization(\n",
        "    max_tokens=max_features,\n",
        "    output_mode='binary',\n",
        "    pad_to_max_tokens=True)\n",
        "\n",
        "train_ds, val_ds, test_ds = final_sets_plot(raw_train_ds, raw_val_ds, raw_test_ds)"
      ],
      "metadata": {
        "id": "ZDg_wcuT-541"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Παράμετροι=600020**, **Είσοδος=30000**, **Έξοδος=20**"
      ],
      "metadata": {
        "id": "7ZYLpxaF_ASj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential([layers.Dense(20, activation='sigmoid', input_shape=(max_features,))])\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "zAdeAt0I-_Sn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Αποτελέσματα 4**\n",
        "Στις **20 εποχές** έχουμε **Accuracy: 0.502** και **Loss: 0.199**\n",
        "\n",
        "Στις **10 εποχές** έχουμε **Accuracy: 0.490** και **Loss: 0.290**\n",
        "\n",
        "Στις **5 εποχές** έχουμε **Accuracy: 0.499** και **Loss: 0.238**\n",
        "\n",
        "Οπότε στις 20 εποχές (που έχουν τρέξει και τα υπόλοιπα) είχαμε το καλύτερο αποτέλεσμα, το οποίο γενικά ίσως θεωρείται χαμηλό σαν accuracy, αλλά είναι σαφώς καλύτερο από τυχαία επιλογή ανάμεσα σε 20 κλάσεις (όπως είπατε)."
      ],
      "metadata": {
        "id": "sKfBRU2N_QY6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss=losses.BinaryCrossentropy(),\n",
        "              optimizer=tf.keras.optimizers.Adam(0.001),\n",
        "              metrics=['accuracy'])\n",
        "epochs = 10\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=epochs)\n",
        "\n",
        "loss, accuracy = model.evaluate(test_ds)\n",
        "\n",
        "print(\"Loss: \", loss)\n",
        "print(\"Accuracy: \", accuracy)"
      ],
      "metadata": {
        "id": "I_JPg8nE_JM7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Ενσωματώσεις λέξεων μόνο με χρήση Title**\n",
        "Έχοντας αλλάξει το output_mode του **vectorize_layer** σε '**int**', και περιορίζοντας τους **τίτλους** σε **250 λέξεις** (έτσι κι αλλιώς είναι μικρότεροι), επιχειρούμε τον τρόπο αναπαράστασης με **word embeddings**."
      ],
      "metadata": {
        "id": "ZrvvfFFGHHr_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_features = 10000\n",
        "sequence_length = 250\n",
        "\n",
        "vectorize_layer = TextVectorization(\n",
        "    max_tokens=max_features,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=sequence_length)\n",
        "\n",
        "train_ds, val_ds, test_ds = final_sets(raw_train_ds, raw_val_ds, raw_test_ds)"
      ],
      "metadata": {
        "id": "LTLZ5ungIuD8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for movie in train_ds.take(1):\n",
        "  print(movie[0])"
      ],
      "metadata": {
        "id": "GCiGWH7SJl4g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ενσωμάτωση λέξεων** με **16 διαστάσεις**."
      ],
      "metadata": {
        "id": "QjWatXjbQ0z1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 16\n",
        "model = tf.keras.Sequential([\n",
        "  layers.Embedding(max_features, embedding_dim),\n",
        "  layers.GlobalAveragePooling1D(),\n",
        "  layers.Dense(20)])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "iWsPtwR3ECXs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Αποτελέσματα 5**\n",
        "Στις **20 εποχές** έχουμε **Accuracy: 0.267** και **Loss: 0.160**, το οποίο είναι πολύ χαμηλό. Όχι ιδιαίτερα χαμηλότερο από την εκπαίδευση με **bag of words** λεξιλογίου ίδιας ποσότητας λέξεων, με **χρήση τίτλου**, όπου είχαμε **Accuracy: 0.35**, αλλά σίγουρα χαμηλότερη. "
      ],
      "metadata": {
        "id": "C523yMksRBfm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss=losses.BinaryCrossentropy(from_logits=True),\n",
        "              optimizer=tf.keras.optimizers.Adam(0.001),\n",
        "              metrics=['accuracy'])\n",
        "epochs = 20\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=epochs)"
      ],
      "metadata": {
        "id": "JMvEn7-bHOkm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss, accuracy = model.evaluate(test_ds)\n",
        "\n",
        "print(\"Loss: \", loss)\n",
        "print(\"Accuracy: \", accuracy)"
      ],
      "metadata": {
        "id": "TpSov1jeMN-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Ενσωματώσεις λέξεων μόνο με χρήση Plot**\n",
        "Τέλος θα δοκιμάσουμε τα ίδια ακριβώς, με χρήση μόνο **σεναρίου**. "
      ],
      "metadata": {
        "id": "v62zT2TKO8T8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_features = 10000\n",
        "sequence_length = 250\n",
        "\n",
        "vectorize_layer = TextVectorization(\n",
        "    max_tokens=max_features,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=sequence_length)\n",
        "\n",
        "train_ds, val_ds, test_ds = final_sets_plot(raw_train_ds, raw_val_ds, raw_test_ds)"
      ],
      "metadata": {
        "id": "oiPAu3ZCO703"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 16\n",
        "model = tf.keras.Sequential([\n",
        "  layers.Embedding(max_features, embedding_dim),\n",
        "  layers.GlobalAveragePooling1D(),\n",
        "  layers.Dense(20)])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "CCSe1PxkPC_v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Αποτελέσματα 6**\n",
        "Πλέον, στις **20 εποχές** έχουμε **Accuracy: 0.433** και **Loss: 0.139**, τα οποία είναι πάλι χαμηλότερα των αντίστοιχων αποτελεσμάτων με χρήση **bag of words** αναπαράστασης, **λεξιλογίου 1000 λέξεων**, αλλά εδώ θα παρατηρήσουμε την μεγάλη διαφορά μεταξύ:\n",
        "\n",
        "**Word embeddings με χρήση τίτλου** και **word embeddings με χρήση σεναρίου**. **Accuracy πρώτου: 0.267**, **Accuracy δεύτερου: 0.433**. Παρατηρούμε πόσο σημαντική είναι η **ποσότητα του text** στα **embeddings**, διαφορά η οποία δεν υπάρχει σε τέτοιο μέγεθος στα αντίστοιχα bag of words αποτελέσματα.\n",
        "\n",
        "Σίγουρα λοιπόν είχαμε **παρόμοιου επιπέδου αποτελέσματα** στις δύο αναπαραστάσεις, αλλά η πραγματική διαφορά υπήρξε στο μέγεθος βελτίωσης των **word embeddings** δίνοντας τους περισσότερο text (**Plot**). "
      ],
      "metadata": {
        "id": "w4wqWdFKR-EZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss=losses.BinaryCrossentropy(from_logits=True),\n",
        "              optimizer=tf.keras.optimizers.Adam(0.001),\n",
        "              metrics=['accuracy'])\n",
        "epochs = 20\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=epochs)\n",
        "\n",
        "loss, accuracy = model.evaluate(test_ds)\n",
        "\n",
        "print(\"Loss: \", loss)\n",
        "print(\"Accuracy: \", accuracy)"
      ],
      "metadata": {
        "id": "GBneD__aPEuc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}