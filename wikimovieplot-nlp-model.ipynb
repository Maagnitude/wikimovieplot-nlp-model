{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNo5zTma7tASm/wKKO5rL/R",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Maagnitude/wikimovieplot-nlp-model/blob/main/wikimovieplot-nlp-model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3η Εργασία** στο μάθημα **Μηχανική Μάθηση και Εφαρμογές**\n",
        "\n",
        "# **Τμήμα Πληροφορικής και Τηλεματικής - Χαροκόπειο Πανεπιστήμιο**\n",
        "\n",
        "# **Καζάζης Γεώργιος - it214124**\n",
        "\n",
        "Στην παρούσα εργασία θα χρησιμοποιήσουμε μεθόδους επεξεργασίας φυσικής γλώσσας."
      ],
      "metadata": {
        "id": "C8kx-GPaYPi3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Modules**\n",
        "Κάνουμε import τις απαραίτητες βιβλιοθήκες:\n",
        "* **pandas** και **numpy** για την διαχείριση των δεδομένων μας.\n",
        "* **re** για την επεξεργασία κειμένου (lowercase κτλ.)\n",
        "* **tensorflow**, και από αυτήν το **keras** καθώς και τα **layers**, **losses**, **preprocessing** για την ανάπτυξη Νευρωνικών Δικτύων, και την εκπαίδευση τους.\n",
        "* **TextVectorization** για να κάνουμε **vectorize** τα **Plot** και **Title** για την χρήση τους στην εκπαίδευση.\n",
        "* Τα **warnings** για να τα φιλτράρουμε, ώστε να μην εμφανίζονται "
      ],
      "metadata": {
        "id": "5S6lZwror07D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "from keras import losses\n",
        "from keras import preprocessing\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(action='ignore')"
      ],
      "metadata": {
        "id": "4P5qV3xhYshF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Dataset**\n",
        "Περνάμε το link από το **public github repo** μου, για το **dataset** μας, σε μορφή **csv**, το οποίο θα δώσουμε στην συνάρτηση για το φόρτωμα των δεδομένων μας."
      ],
      "metadata": {
        "id": "VoalJpu0avWb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://raw.githubusercontent.com/Maagnitude/wikimovieplot-nlp-model/main/dataset.csv\""
      ],
      "metadata": {
        "id": "CEruND9PauoA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Text preprocessing**\n",
        "Με αυτή την συνάρτηση θα επεξεργαστούμε τα Title και Plot, ώστε να έχουν όλα πεζά γράμματα, να μην έχουν ειδικούς χαρακτήρες και σημεία στίξης."
      ],
      "metadata": {
        "id": "flTnjrrhtn00"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    text = text.lower() # convert text to lowercase\n",
        "    text = re.sub(r'[^\\w\\s]', '', text) # remove punctuation\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    return text"
      ],
      "metadata": {
        "id": "nt6skjaysvuh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Βασική συνάρτηση preprocessing**\n",
        "Εδώ υλοποιείται το μεγαλύτερο κομμάτι του notebook. \n",
        "\n",
        "Αρχικά φορτώνεται το dataset, ανακατεύονται οι γραμμές του με την sample, χωρίζεται σε train, validate και test set (έχοντας υπολογίσει τα ποσοστά του καθενός).\n",
        "\n",
        "Εν συνεχεία, γίνεται **One-hot encoding** στα features '**Origin/Ethnicity**' και '**Genre**' με την χρήση της **pd.get_dummies** (σε κάθε **set**), τα οποία περνιούνται με τα κατάλληλα **keys** στα **dictionaries** που δημιουργούμε στο επόμενο βήμα. Από το shape[1] ενός εκ των τριών genre_dummies, παίρνουμε τις 20 διαφορετικές τιμές του feature 'Genre' ώστε να το χρησιμοποιήσουμε αργότερα.\n",
        "\n",
        "Πριν περάσουμε τα '**Title**' και '**Plot**' στα **dictionaries**, τα περνάμε από την παραπάνω συνάρτηση επεξεργασίας κειμένου με την χρήση της **apply()**. \n",
        "\n",
        "Δημιουργούμε τα **dictionaries** και ύστερα τα περνάμε με την **from_tensor_slices()** (των **tf.data.Dataset**) στα Dataset object (**raw_train_ds**, **raw_val_ds**, **raw_test_ds**), και τέλος τα επιστρέφουμε. "
      ],
      "metadata": {
        "id": "TmjY6-Jut9JH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data_wiki(file_path, batch_size=32, p_train=0.65, p_val=0.15):\n",
        "    \n",
        "    df = pd.read_csv(url)\n",
        "    df = df.sample(frac=1, random_state=42)\n",
        "\n",
        "    n = df.shape[0]\n",
        "    n_train = int(n * p_train)\n",
        "    n_val = int(n * p_val)\n",
        "    n_test = n - n_train - n_val\n",
        "\n",
        "    df_train, df_val, df_test = df.iloc[:n_train], df.iloc[n_train:n_train+n_val],\\\n",
        "                                                          df.iloc[n_train+n_val:]\n",
        "\n",
        "    origin_dummies1 = pd.get_dummies(df_train['Origin/Ethnicity'])\n",
        "    genre_dummies1 = pd.get_dummies(df_train['Genre'])\n",
        "\n",
        "    origin_dummies2 = pd.get_dummies(df_val['Origin/Ethnicity'])\n",
        "    genre_dummies2 = pd.get_dummies(df_val['Genre'])\n",
        "\n",
        "    origin_dummies3 = pd.get_dummies(df_test['Origin/Ethnicity'])\n",
        "    genre_dummies3 = pd.get_dummies(df_test['Genre'])\n",
        "\n",
        "    n_labels = genre_dummies1.shape[1]\n",
        "\n",
        "    df['Title'] = df['Title'].apply(preprocess_text)\n",
        "    df['Plot'] = df['Plot'].apply(preprocess_text)  \n",
        "                                                            \n",
        "    train_data = {'Title': df_train['Title'].values,\n",
        "                  'Origin': origin_dummies1.values.astype(np.float64),\n",
        "                  'Plot': df_train['Plot'].values,\n",
        "                  'Genre': genre_dummies1.values.astype(np.float64)}\n",
        "\n",
        "    val_data = {'Title': df_val['Title'].values,\n",
        "                'Origin': origin_dummies2.values.astype(np.float64),\n",
        "                'Plot': df_val['Plot'].values,\n",
        "                'Genre': genre_dummies2.values.astype(np.float64)}            \n",
        "\n",
        "    test_data = {'Title': df_test['Title'].values,\n",
        "                 'Origin': origin_dummies3.values.astype(np.float64),\n",
        "                 'Plot': df_test['Plot'].values,\n",
        "                 'Genre': genre_dummies3.values.astype(np.float64)}\n",
        "\n",
        "    raw_train_ds = tf.data.Dataset.from_tensor_slices(train_data).batch(batch_size)\n",
        "    raw_val_ds = tf.data.Dataset.from_tensor_slices(val_data).batch(batch_size)\n",
        "    raw_test_ds = tf.data.Dataset.from_tensor_slices(test_data).batch(batch_size)\n",
        "\n",
        "    return raw_train_ds, raw_val_ds, raw_test_ds, n_labels"
      ],
      "metadata": {
        "id": "51196DXOH9PA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_train_ds, raw_val_ds, raw_test_ds, n_labels = load_data_wiki(url)"
      ],
      "metadata": {
        "id": "5DpaPDLAJzXM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Έλεγχος των δεδομένων**\n",
        "Εδώ θα τσεκάρουμε τον **αριθμό των διαφορετικών Genre**, καθώς και αν τα **Title**, **Genre**, **Origin** και **Plot**, της πρώτης ταινίας του πρώτου batch, έχουν περαστεί σωστά (**Title** και **Plot** επεξεργασμένα σωστά - **Origin** και **Genre** **one-hot encoded**)"
      ],
      "metadata": {
        "id": "8i3H0kTwwbTR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_labels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hi5K3q1eKiv0",
        "outputId": "93c1db91-579a-4864-f761-571ed7e335dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20"
            ]
          },
          "metadata": {},
          "execution_count": 320
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for movie in raw_train_ds.take(1):\n",
        "  print('First movie, from the first batch:')\n",
        "  print('Title (lowercase etc.): ', movie['Title'][0].numpy())\n",
        "  print('Genre (One-hot): ', movie['Genre'][0].numpy())\n",
        "  print('Origin (One-hot): ', movie['Origin'][0].numpy())\n",
        "  print('Plot (lowercase etc.): ', movie['Plot'][0].numpy())\n"
      ],
      "metadata": {
        "id": "Hwv4KMBdMRie",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18cc7ada-0765-4b65-f1df-845d2d562740"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First movie, from the first batch:\n",
            "Title (lowercase etc.):  b'creature with the atom brain'\n",
            "Genre (One-hot):  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Origin (One-hot):  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Plot (lowercase etc.):  b'a hulking zombie breaks into a mansion and kills a gangster named hennesy the blood stains left behind at the crime scene are radioactive and the fingerprints of the killer are of a man who had died days before the murder the police are baffled gangster boss frank buchanan who had been forced to flee the united states before he was deported was betrayed by members of his own underworld gang while traveling in europe he finds exnazi scientist wilhelm steigg gaye who is trying to reanimate the dead in order to provide a menial labor pool that is easily exploited buchanan funds the research and brings the scientist to america with the unstated goal of sending steiggs zombies out to murder those who ousted him one by one they are killed in the same fashion the police eventually discover the common connection between the murdered gang members and buchanan they try to put the remaining three into protective custody but buchanan uses a reanimated dead cop to kill one of them and a reanimated dead police captain to kill the remaining two when the zombie captain is captured police doctor dr chet walker denning discovers an atomicpowered remote control brain implant and deduces what has been going on police and army troops converge on buchanans leadlined mansion and he sends out his unkillable zombies to battle them walker however is able to get into the mansion and smash the atomicpowered equipment that controls the zombies after doing so they all collapse buchanan is about to shoot walker but the stillanimated zombie police captain now under walkers control grabs and strangles buchanan before he can fire a shot'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Πρώτο Vectorization**\n",
        "Εδώ θα υλοποιήσουμε μία διαδικασία η οποία κάνει vectorize τα **Title** και **Plot**, ώστε να δημιουργηθεί μια δυαδική αναπαράσταση των λέξεων (**bag of words**) με λεξιλόγιο (αριθμό λέξεων) που θα ορίσουμε εμείς.\n",
        "\n",
        "* Το πρώτο λεξιλόγιο θα έχει **500 λέξεις** (**max_features**). \n",
        "* Το **output** θα είναι **binary** (1 αν υπάρχει η λέξη, 0 αν δεν υπάρχει)\n",
        "* Με το **pad_to_max_tokens**, βάζοντας το **True**, του λέμε να συμπληρώσει με **μηδενικά** έως ότου οι λέξεις να είναι όσο το **max_features** που θα του δώσουμε."
      ],
      "metadata": {
        "id": "eJfZZrNYyage"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_features = 500\n",
        "\n",
        "vectorize_layer = TextVectorization(\n",
        "    max_tokens=max_features,\n",
        "    output_mode='binary',\n",
        "    pad_to_max_tokens=True)"
      ],
      "metadata": {
        "id": "abj6ztx1rQJS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorize_text(text):\n",
        "  return vectorize_layer(tf.expand_dims(text, -1))"
      ],
      "metadata": {
        "id": "a2BNLFC-Z_EW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Χρήση του Title**\n",
        "Για αρχή χρησιμοποιούμε μόνο το **title** ως είσοδο (χωρίς το Plot), και το δίνουμε όπως και το κάνουμε **adapt** στο **vectorize_layer**, ώστε παρακάτω να γίνει **vectorized** πριν δωθεί στα τελικά sets."
      ],
      "metadata": {
        "id": "IPiHGP6JzqgU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def final_sets(raw_train_ds, raw_val_ds, raw_test_ds):\n",
        "    train_titles = raw_train_ds.map(lambda x: x['Title'])\n",
        "    vectorize_layer.adapt(train_titles)\n",
        "    val_titles = raw_val_ds.map(lambda x: x['Title'])\n",
        "    vectorize_layer.adapt(val_titles)\n",
        "    test_titles = raw_test_ds.map(lambda x: x['Title'])\n",
        "    vectorize_layer.adapt(test_titles)\n",
        "\n",
        "    train_ds = raw_train_ds.map(lambda x: (vectorize_text(x['Title']), x['Genre']))\n",
        "    val_ds = raw_val_ds.map(lambda x: (vectorize_text(x['Title']), x['Genre']))\n",
        "    test_ds = raw_test_ds.map(lambda x: (vectorize_text(x['Title']), x['Genre']))\n",
        "\n",
        "    return train_ds, val_ds, test_ds"
      ],
      "metadata": {
        "id": "CnGLTietZ44c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds, val_ds, test_ds = final_sets(raw_train_ds, raw_val_ds, raw_test_ds)"
      ],
      "metadata": {
        "id": "t56Y9SeVeYZf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Πρώτο μοντέλο** \n",
        "Γίνεται η εκπαίδευση του πρώτου μοντέλου με χρήση μόνο του **Title**, για την πρόβλεψη του **Genre**.\n",
        "\n",
        "**10020 παράμετροι** - **Είσοδος = 500**,  **Έξοδος = 20**"
      ],
      "metadata": {
        "id": "phcAspLw3MIv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential([layers.Dense(20, activation='sigmoid', input_shape=(max_features,))])\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QojWc9OIYnbr",
        "outputId": "4d47e0fa-b4ca-46af-e394-0129885167b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_29\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_29 (Dense)            (None, 20)                10020     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 10,020\n",
            "Trainable params: 10,020\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Αποτελέσματα**\n",
        "Χρησιμοποιήθηκε ως **optimizer** η μέθοδος **Adam**, με **learning rate** **0.001**, και **default** οι υπόλοιπες παράμετροι.\n",
        "\n",
        "Στις 10 εποχές είχαμε **Accuracy: 0.303**, και **Loss: 0.161**\n",
        "\n",
        "Στις 20 εποχές είχαμε **Accuracy: 0.316**, και **Loss: 0.158**\n",
        "\n",
        "**Δεν δοκιμάστηκε** σε παραπάνω εποχές μιας και θα το ξανατρέξουμε για **λεξιλόγιο 10000 λέξεων** ώστε να δούμε αν υπάρχει κάποια διαφορά. "
      ],
      "metadata": {
        "id": "pjiBU_6a3pEP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss=losses.BinaryCrossentropy(),\n",
        "              optimizer=tf.keras.optimizers.Adam(0.001),\n",
        "              metrics=['accuracy'])\n",
        "epochs = 20\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=epochs)\n",
        "\n",
        "loss, accuracy = model.evaluate(test_ds)\n",
        "\n",
        "print(\"Loss: \", loss)\n",
        "print(\"Accuracy: \", accuracy)"
      ],
      "metadata": {
        "id": "2byKIQvmYqnT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Λεξιλόγιο 10000 λέξεων**\n",
        "Αυξάνουμε τις λέξεις στο **bag of words**, και ξαναδημιουργούμε τα **sets**. Ύστερα εκπαιδεύουμε πάλι το μοντέλο μας και το τρέχουμε με τις **ίδιες παραμέτρους** για **20 εποχές**, για να το συγκρίνουμε με το **πρώτο**.\n",
        "\n",
        "Πλέον, έχουμε **200020 παραμέτρους**, **Είσοδο=10000**, **Έξοδο=20**."
      ],
      "metadata": {
        "id": "uZDFhmPs7CCN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_features = 10000\n",
        "vectorize_layer = TextVectorization(\n",
        "    max_tokens=max_features,\n",
        "    output_mode='binary',\n",
        "    pad_to_max_tokens=True)\n",
        "\n",
        "train_ds, val_ds, test_ds = final_sets(raw_train_ds, raw_val_ds, raw_test_ds)"
      ],
      "metadata": {
        "id": "ztD-Clyc4_GZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential([layers.Dense(20, activation='sigmoid', input_shape=(max_features,))])\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cVY_j5ip6lCt",
        "outputId": "acac6b74-9b3a-469a-b516-84e7af26fc73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_31\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_31 (Dense)            (None, 20)                200020    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 200,020\n",
            "Trainable params: 200,020\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Αποτελέσματα 2**\n",
        "Στις **20 εποχές** έχουμε **Accuracy: 0.355** και **Loss: 0.159**.\n",
        "\n",
        "\n",
        "**Απάντηση στην ερώτηση**:\n",
        "\n",
        "Τα έχει πάει σίγουρα καλύτερα από το πρώτο, αλλά μεγάλη διαφορά δεν υπάρχει παρ' όλο που αυξήθηκαν οι λέξεις στο λεξιλόγιο, κι αυτό οφείλεται, κατά πάσα πιθανότητα, στο ότι με τον τίτλο μόνο, το μοντέλο δεν μπορεί να πάρει τις απαραίτητες πληροφορίες στην εκπαίδευση του. "
      ],
      "metadata": {
        "id": "d3Oq2R_E724W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss=losses.BinaryCrossentropy(),\n",
        "              optimizer=tf.keras.optimizers.Adam(0.001),\n",
        "              metrics=['accuracy'])\n",
        "epochs = 20\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=epochs)\n",
        "\n",
        "loss, accuracy = model.evaluate(test_ds)\n",
        "\n",
        "print(\"Loss: \", loss)\n",
        "print(\"Accuracy: \", accuracy)"
      ],
      "metadata": {
        "id": "BGTTFDYQ6pKP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Χρήση του Plot**\n",
        "Τώρα με την χρήση της συνάρτησης **final_sets_plot**, θα δημιουργήσουμε νέα sets τα οποία θα έχουν μόνο το **Plot**, ώστε να δοκιμάσουμε να εκπαιδεύσουμε ένα μοντέλο με τα **σενάρια των ταινιών** και να ελέγξουμε τα αποτελέσματα του."
      ],
      "metadata": {
        "id": "-A946HbZ9Xxi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def final_sets_plot(raw_train_ds, raw_val_ds, raw_test_ds):\n",
        "    train_titles = raw_train_ds.map(lambda x: x['Plot'])\n",
        "    vectorize_layer.adapt(train_titles)\n",
        "    val_titles = raw_val_ds.map(lambda x: x['Plot'])\n",
        "    vectorize_layer.adapt(val_titles)\n",
        "    test_titles = raw_test_ds.map(lambda x: x['Plot'])\n",
        "    vectorize_layer.adapt(test_titles)\n",
        "\n",
        "    train_ds = raw_train_ds.map(lambda x: (vectorize_text(x['Plot']), x['Genre']))\n",
        "    val_ds = raw_val_ds.map(lambda x: (vectorize_text(x['Plot']), x['Genre']))\n",
        "    test_ds = raw_test_ds.map(lambda x: (vectorize_text(x['Plot']), x['Genre']))\n",
        "\n",
        "    return train_ds, val_ds, test_ds"
      ],
      "metadata": {
        "id": "IDMWYUT08yLL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Θα χρησιμοποιήσουμε ένα **λεξιλόγιο 1000 λέξεων**, και όλες τις υπόλοιπες παραμέτρους ίδιες."
      ],
      "metadata": {
        "id": "ts_BTD7G9sx7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_features = 1000\n",
        "vectorize_layer = TextVectorization(\n",
        "    max_tokens=max_features,\n",
        "    output_mode='binary',\n",
        "    pad_to_max_tokens=True)\n",
        "\n",
        "train_ds, val_ds, test_ds = final_sets_plot(raw_train_ds, raw_val_ds, raw_test_ds)"
      ],
      "metadata": {
        "id": "G_G5Azwh8_4E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Παράμετροι=20020**, **Είσοδος=1000**, **Έξοδος=20**"
      ],
      "metadata": {
        "id": "MVoWKO3K-BOb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential([layers.Dense(20, activation='sigmoid', input_shape=(max_features,))])\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o_TXOvmh9F1-",
        "outputId": "b95a581a-258a-4e86-8328-677651ca8c6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_32\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_32 (Dense)            (None, 20)                20020     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 20,020\n",
            "Trainable params: 20,020\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Αποτελέσματα 3**\n",
        "Έχουμε **καθαρή βελτίωση** από τα προηγούμενα, μιας και πλέον έχει ως είσοδο τα **σενάρια** των ταινιών από τα οποία μπορεί να αντλήσει σαφώς περισσότερες πληροφορίες, κι έτσι με **λεξιλόγιο 1000 λέξεων** τα πήγε πολύ καλύτερα από ότι τα είχε πάει το μοντέλο που εκπαιδεύτηκε στους **τίτλους** των ταινιών με **λεξιλόγιο 10000 λέξεων**.\n",
        "\n",
        "**Accuracy: 0.454**, **Loss: 0.139**\n",
        "\n",
        "Παρατηρούμε όμως ότι μετά την **7η εποχή** το **val_loss** μόνο αυξάνεται και γενικά ενώ το μοντέλο πετυχαίνει **accuracy** στο train set τιμές υψηλότερες του **0.57**, δεν γενικεύει αποτελεσματικό, περίπτωση **υπερεκπαίδευσης**. Θα γίνει μία δοκιμή και στις **10 εποχές**."
      ],
      "metadata": {
        "id": "fM3wcUhy-KfA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss=losses.BinaryCrossentropy(),\n",
        "              optimizer=tf.keras.optimizers.Adam(0.001),\n",
        "              metrics=['accuracy'])\n",
        "epochs = 20\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=epochs)\n",
        "\n",
        "loss, accuracy = model.evaluate(test_ds)\n",
        "\n",
        "print(\"Loss: \", loss)\n",
        "print(\"Accuracy: \", accuracy)"
      ],
      "metadata": {
        "id": "4Y8aqqA-9Jkc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Εκπαίδευση με 10 εποχές για αποφυγή υπερεκπαίδευσης**\n",
        "Πράγματι έχουμε **Accuracy: 0.463**, και **Loss: 0.131**. Υπάρχει μικρή βελτίωση, όχι αξιοσημείωτη."
      ],
      "metadata": {
        "id": "KNoZ_kSLBH8B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_features = 1000\n",
        "vectorize_layer = TextVectorization(\n",
        "    max_tokens=max_features,\n",
        "    output_mode='binary',\n",
        "    pad_to_max_tokens=True)\n",
        "\n",
        "train_ds, val_ds, test_ds = final_sets_plot(raw_train_ds, raw_val_ds, raw_test_ds)"
      ],
      "metadata": {
        "id": "VY_dfN5bBViE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential([layers.Dense(20, activation='sigmoid', input_shape=(max_features,))])\n",
        "model.summary()\n",
        "model.compile(loss=losses.BinaryCrossentropy(),\n",
        "              optimizer=tf.keras.optimizers.Adam(0.001),\n",
        "              metrics=['accuracy'])\n",
        "epochs = 10\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=epochs)\n",
        "\n",
        "loss, accuracy = model.evaluate(test_ds)\n",
        "\n",
        "print(\"Loss: \", loss)\n",
        "print(\"Accuracy: \", accuracy)"
      ],
      "metadata": {
        "id": "JZOlOLbkBE8N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_features = 30000\n",
        "vectorize_layer = TextVectorization(\n",
        "    max_tokens=max_features,\n",
        "    output_mode='binary',\n",
        "    pad_to_max_tokens=True)\n",
        "\n",
        "train_ds, val_ds, test_ds = final_sets_plot(raw_train_ds, raw_val_ds, raw_test_ds)"
      ],
      "metadata": {
        "id": "ZDg_wcuT-541"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Παράμετροι=600020**, **Είσοδος=30000**, **Έξοδος=20**"
      ],
      "metadata": {
        "id": "7ZYLpxaF_ASj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential([layers.Dense(20, activation='sigmoid', input_shape=(max_features,))])\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zAdeAt0I-_Sn",
        "outputId": "ff89de3d-10c8-4f8b-a816-84ecfd0b0a1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_36\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_36 (Dense)            (None, 20)                600020    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 600,020\n",
            "Trainable params: 600,020\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Αποτελέσματα 4**\n",
        "Στις **20 εποχές** έχουμε **Accuracy: 0.502** και **Loss: 0.199**\n",
        "\n",
        "Στις **10 εποχές** έχουμε **Accuracy: 0.490** και **Loss: 0.290**\n",
        "\n",
        "Στις **5 εποχές** έχουμε **Accuracy: 0.499** και **Loss: 0.238**\n",
        "\n",
        "Οπότε στις 20 εποχές (που έχουν τρέξει και τα υπόλοιπα) είχαμε το καλύτερο αποτέλεσμα, το οποίο γενικά ίσως θεωρείται χαμηλό σαν accuracy, αλλά είναι σαφώς καλύτερο από τυχαία επιλογή ανάμεσα σε 20 κλάσεις (όπως είπατε)."
      ],
      "metadata": {
        "id": "sKfBRU2N_QY6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss=losses.BinaryCrossentropy(),\n",
        "              optimizer=tf.keras.optimizers.Adam(0.001),\n",
        "              metrics=['accuracy'])\n",
        "epochs = 10\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=epochs)\n",
        "\n",
        "loss, accuracy = model.evaluate(test_ds)\n",
        "\n",
        "print(\"Loss: \", loss)\n",
        "print(\"Accuracy: \", accuracy)"
      ],
      "metadata": {
        "id": "I_JPg8nE_JM7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Ενσωματώσεις λέξεων μόνο με χρήση Title**\n",
        "Έχοντας αλλάξει το output_mode του **vectorize_layer** σε '**int**', και περιορίζοντας τους **τίτλους** σε **250 λέξεις** (έτσι κι αλλιώς είναι μικρότεροι), επιχειρούμε τον τρόπο αναπαράστασης με **word embeddings**."
      ],
      "metadata": {
        "id": "ZrvvfFFGHHr_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_features = 10000\n",
        "sequence_length = 250\n",
        "\n",
        "vectorize_layer = TextVectorization(\n",
        "    max_tokens=max_features,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=sequence_length)\n",
        "\n",
        "train_ds, val_ds, test_ds = final_sets(raw_train_ds, raw_val_ds, raw_test_ds)"
      ],
      "metadata": {
        "id": "LTLZ5ungIuD8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for movie in train_ds.take(1):\n",
        "  print(movie[0])"
      ],
      "metadata": {
        "id": "GCiGWH7SJl4g",
        "outputId": "bd348d5d-9db6-44fc-fab7-8912d4ab596a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[374  22   2 ...   0   0   0]\n",
            " [207   3   1 ...   0   0   0]\n",
            " [  2  23  22 ...   0   0   0]\n",
            " ...\n",
            " [  1   0   0 ...   0   0   0]\n",
            " [ 11  68  58 ...   0   0   0]\n",
            " [  1   0   0 ...   0   0   0]], shape=(32, 250), dtype=int64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ενσωμάτωση λέξεων** με **16 διαστάσεις**."
      ],
      "metadata": {
        "id": "QjWatXjbQ0z1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 16\n",
        "model = tf.keras.Sequential([\n",
        "  layers.Embedding(max_features, embedding_dim),\n",
        "  layers.GlobalAveragePooling1D(),\n",
        "  layers.Dense(20)])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iWsPtwR3ECXs",
        "outputId": "4f254487-26e2-4c31-91af-07536e8a7d74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_51\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_14 (Embedding)    (None, None, 16)          160000    \n",
            "                                                                 \n",
            " global_average_pooling1d_10  (None, 16)               0         \n",
            "  (GlobalAveragePooling1D)                                       \n",
            "                                                                 \n",
            " dense_51 (Dense)            (None, 20)                340       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 160,340\n",
            "Trainable params: 160,340\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Αποτελέσματα 5**\n",
        "Στις **20 εποχές** έχουμε **Accuracy: 0.267** και **Loss: 0.160**, το οποίο είναι πολύ χαμηλό. Όχι ιδιαίτερα χαμηλότερο από την εκπαίδευση με **bag of words** λεξιλογίου ίδιας ποσότητας λέξεων, με **χρήση τίτλου**, όπου είχαμε **Accuracy: 0.35**, αλλά σίγουρα χαμηλότερη. "
      ],
      "metadata": {
        "id": "C523yMksRBfm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss=losses.BinaryCrossentropy(from_logits=True),\n",
        "              optimizer=tf.keras.optimizers.Adam(0.001),\n",
        "              metrics=['accuracy'])\n",
        "epochs = 20\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=epochs)"
      ],
      "metadata": {
        "id": "JMvEn7-bHOkm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss, accuracy = model.evaluate(test_ds)\n",
        "\n",
        "print(\"Loss: \", loss)\n",
        "print(\"Accuracy: \", accuracy)"
      ],
      "metadata": {
        "id": "TpSov1jeMN-9",
        "outputId": "712e91ef-c42d-405f-eca4-f15cacd7e1b6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "144/144 [==============================] - 0s 3ms/step - loss: 0.1609 - accuracy: 0.2673\n",
            "Loss:  0.16094480454921722\n",
            "Accuracy:  0.2672601044178009\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Ενσωματώσεις λέξεων μόνο με χρήση Plot**\n",
        "Τέλος θα δοκιμάσουμε τα ίδια ακριβώς, με χρήση μόνο **σεναρίου**. "
      ],
      "metadata": {
        "id": "v62zT2TKO8T8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_features = 10000\n",
        "sequence_length = 250\n",
        "\n",
        "vectorize_layer = TextVectorization(\n",
        "    max_tokens=max_features,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=sequence_length)\n",
        "\n",
        "train_ds, val_ds, test_ds = final_sets_plot(raw_train_ds, raw_val_ds, raw_test_ds)"
      ],
      "metadata": {
        "id": "oiPAu3ZCO703"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 16\n",
        "model = tf.keras.Sequential([\n",
        "  layers.Embedding(max_features, embedding_dim),\n",
        "  layers.GlobalAveragePooling1D(),\n",
        "  layers.Dense(20)])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "CCSe1PxkPC_v",
        "outputId": "9e7bdc97-4638-46c0-d7e9-640ce066b865",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_52\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_15 (Embedding)    (None, None, 16)          160000    \n",
            "                                                                 \n",
            " global_average_pooling1d_11  (None, 16)               0         \n",
            "  (GlobalAveragePooling1D)                                       \n",
            "                                                                 \n",
            " dense_52 (Dense)            (None, 20)                340       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 160,340\n",
            "Trainable params: 160,340\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Αποτελέσματα 6**\n",
        "Πλέον, στις **20 εποχές** έχουμε **Accuracy: 0.433** και **Loss: 0.139**, τα οποία είναι πάλι χαμηλότερα των αντίστοιχων αποτελεσμάτων με χρήση **bag of words** αναπαράστασης, **λεξιλογίου 1000 λέξεων**, αλλά εδώ θα παρατηρήσουμε την μεγάλη διαφορά μεταξύ:\n",
        "\n",
        "**Word embeddings με χρήση τίτλου** και **word embeddings με χρήση σεναρίου**. **Accuracy πρώτου: 0.267**, **Accuracy δεύτερου: 0.433**. Παρατηρούμε πόσο σημαντική είναι η **ποσότητα του text** στα **embeddings**, διαφορά η οποία δεν υπάρχει σε τέτοιο μέγεθος στα αντίστοιχα bag of words αποτελέσματα.\n",
        "\n",
        "Σίγουρα λοιπόν είχαμε **παρόμοιου επιπέδου αποτελέσματα** στις δύο αναπαραστάσεις, αλλά η πραγματική διαφορά υπήρξε στο μέγεθος βελτίωσης των **word embeddings** δίνοντας τους περισσότερο text (**Plot**). "
      ],
      "metadata": {
        "id": "w4wqWdFKR-EZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss=losses.BinaryCrossentropy(from_logits=True),\n",
        "              optimizer=tf.keras.optimizers.Adam(0.001),\n",
        "              metrics=['accuracy'])\n",
        "epochs = 20\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=epochs)\n",
        "\n",
        "loss, accuracy = model.evaluate(test_ds)\n",
        "\n",
        "print(\"Loss: \", loss)\n",
        "print(\"Accuracy: \", accuracy)"
      ],
      "metadata": {
        "id": "GBneD__aPEuc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}