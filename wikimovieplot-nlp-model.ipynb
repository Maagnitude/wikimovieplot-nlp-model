{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOUXuMyM4sEFtoppcRfqpmv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Maagnitude/wikimovieplot-nlp-model/blob/main/wikimovieplot-nlp-model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3η Εργασία** στο μάθημα **Μηχανική Μάθηση και Εφαρμογές**\n",
        "\n",
        "# **Τμήμα Πληροφορικής και Τηλεματικής - Χαροκόπειο Πανεπιστήμιο**\n",
        "\n",
        "# **Καζάζης Γεώργιος - it214124**\n",
        "\n",
        "Στην παρούσα εργασία θα χρησιμοποιήσουμε μεθόδους επεξεργασίας φυσικής γλώσσας."
      ],
      "metadata": {
        "id": "C8kx-GPaYPi3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "import shutil\n",
        "import string\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import losses\n",
        "from tensorflow.keras import preprocessing\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(action='ignore')"
      ],
      "metadata": {
        "id": "4P5qV3xhYshF"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "VoalJpu0avWb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://raw.githubusercontent.com/Maagnitude/wikimovieplot-nlp-model/main/dataset.csv\"\n",
        "# df = pd.read_csv(url)\n",
        "# df = df.sample(frac=1, random_sra)\n",
        "# df.head()\n",
        "\n",
        "# url = \"https://github.com/Maagnitude/wikimovieplot-nlp-model/blob/main/dataset.csv.gz\"\n",
        "# dataset = tf.keras.utils.get_file(\"dataset.csv.gz\", url, untar=True, cache_dir='.', cache_subdir='')\n",
        "# dataset_dir = os.path.join(os.path.dirname(dataset), 'dataset')\n",
        "# os.listdir(dataset_dir)\n",
        "# train_dir = os.path.join(dataset_dir, 'train')\n",
        "# os.listdir(train_dir)\n"
      ],
      "metadata": {
        "id": "CEruND9PauoA"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    text = text.lower() # convert text to lowercase\n",
        "    text = re.sub(r'[^\\w\\s]', '', text) # remove punctuation\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    return text"
      ],
      "metadata": {
        "id": "nt6skjaysvuh"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data_wiki(file_path, batch_size=32, p_train=0.65, p_val=0.15):\n",
        "    \n",
        "    df = pd.read_csv(url)\n",
        "    df = df.sample(frac=1, random_state=42)\n",
        "\n",
        "    origin_dummies = pd.get_dummies(df['Origin/Ethnicity'])\n",
        "    genre_dummies = pd.get_dummies(df['Genre'])\n",
        "    n_labels = genre_dummies.shape[1]\n",
        "\n",
        "    df['Title'] = df['Title'].apply(preprocess_text)\n",
        "    df['Plot'] = df['Plot'].apply(preprocess_text)\n",
        "\n",
        "    df = df.drop('Origin/Ethnicity', axis='columns')\n",
        "    df = df.drop('Genre', axis='columns')\n",
        "    df = pd.concat([df, origin_dummies, genre_dummies], axis=1)\n",
        "\n",
        "    n = df.shape[0]\n",
        "    n_train = int(n * p_train)\n",
        "    n_val = int(n * p_val)\n",
        "    n_test = n - n_train - n_val\n",
        "\n",
        "    df_train, df_val, df_test = df.iloc[:n_train], df.iloc[n_train:n_train+n_val],\\\n",
        "                                                          df.iloc[n_train+n_val:]\n",
        "                                    \n",
        "    train_data = {'Title': df_train['Title'].values,\n",
        "                  'Origin/Ethnicity': df_train.filter(like='Origin/Ethnicity').values.astype(np.float64),\n",
        "                  'Plot': df_train['Plot'].values}\n",
        "    train_labels = df_train.filter(like='Genre').values.astype(np.float64)\n",
        "\n",
        "    val_data = {'Title': df_val['Title'].values,\n",
        "                'Origin/Ethnicity': df_val.filter(like='Origin/Ethnicity').values.astype(np.float64),\n",
        "                'Plot': df_val['Plot'].values}\n",
        "    val_labels = df_train.filter(like='Genre').values.astype(np.float64)            \n",
        "\n",
        "    test_data = {'Title': df_test['Title'].values,\n",
        "                 'Origin/Ethnicity': df_test.filter(like='Origin/Ethnicity').values.astype(np.float64),\n",
        "                 'Plot': df_test['Plot'].values}\n",
        "    test_labels = df_train.filter(like='Genre').values.astype(np.float64)\n",
        "\n",
        "    raw_train_ds = tf.data.Dataset.from_tensor_slices(train_data, train_labels).batch(batch_size)\n",
        "    raw_val_ds = tf.data.Dataset.from_tensor_slices(val_data, val_labels).batch(batch_size)\n",
        "    raw_test_ds = tf.data.Dataset.from_tensor_slices(test_data, test_labels).batch(batch_size)\n",
        "\n",
        "    return raw_train_ds, raw_val_ds, raw_test_ds, n_labels"
      ],
      "metadata": {
        "id": "51196DXOH9PA"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_train_ds, raw_val_ds, raw_test_ds, n_labels = load_data_wiki(url)"
      ],
      "metadata": {
        "id": "5DpaPDLAJzXM"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_labels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hi5K3q1eKiv0",
        "outputId": "2f582cb7-8de7-40e0-d091-fd3a3790a34a"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20"
            ]
          },
          "metadata": {},
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for element in raw_train_ds.take(1):\n",
        "  print(element)"
      ],
      "metadata": {
        "id": "Hwv4KMBdMRie"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}